---
title: "Project1"
author: "Uday Dutt"
date: "October 27, 2017"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
runtime: shiny
resource_files:
- .Renviron
---
![](airbnb2.png)
```{r setup, include=FALSE}
#Load the required library
library(MASS)
library(car)
library(leaps)
library(ggplot2)
library(class)
library(glmnet)
library(httpuv)
library(caTools)

#Define Data Location
srcFile <- 'clean_listings_summary.csv'

# Read data in comma-delimited file
rrdata <- read.csv(srcFile, header=T)
rrdata$neighbourhood<-factor(rrdata$neighbourhood) #changing to factor
mydata<-rrdata[c(1,6:12,14:16)] #New df with elected variables
attach(mydata)
knitr::opts_chunk$set(echo = TRUE)
```
#__AirBnB in Austin__
###__Introduction__
  This project will explore some data on AirBnB in Austin. Airbnb is an online marketplace and hospitality service, enabling people to lease or rent short-term lodging. It has over 3,000,000 lodging listings in 65,000 cities and 191 countries, and the cost of lodging is set by the host.Specifically, this project will cover the modeling of a dataset called "AirBnB in Austin" ([dataset](https://data.world/kurtakranz/s-17-dv-final-project/workspace/file?filename=clean_listings_summary.csv)) by using linear regression and classification models. Afterwards, we will explore the  results of these models with models derived from using Subset Selection and Shrinkage Methods. Then, we will turn the response variable into a categorical variable and attempt to do Logistic Regression, Lnear Discriminant Analysis, Quadratic Discriminant Analysis, and K-Nearest Neighbors.


####__Summary of the Dataset:__

* neighbourhood
    + zip code of the location

* latitude
    + latitude of the location

* longitude
    + longitude of the location

* room type
    + room type of the location

* price
    + price that must be paid if you stay at the location

* minimum nights
    + minimum nights you must stay at the location

* number of reviews
    + total number of reviews the location has received

* number of reviews per month
    + number of reviews the location receives per month

* calcuated host listings count
    + the calculated value of the amount the host lists the location

* number of days available per year("availability")
    + number of days per year the location is available



 Specifically, this project will aim to predict price as the response variable, using the rest of the variables as explanatory variables:

  price~neighbourhood+latitude*longitude+roomtype+minimumnights+numberofreviews+numberofreviewspermonth+availability365

#__Preliminary Analysis__
Before jumping blindy into regression, the data analyst must first inspect the data and play with it.We attempted to understand the data by plotting various bivariate scatter plots between the dependent and each independent variables taking one at a time. The main objective to understand the following elements:

* Presence of outliers: Outliers should be removed from the dataset in order to reduced bias/skewness of the dataset. 

* Normality of the dependent variable: The dependent variable should be normally distributed, as that is a condition that is necessary for Simple Linear and Multivariate regression, LDA, and QDA.

* Linear or non-linear relationships based on changes in variability: we want to identify relationships between certain variables and the price response variable to help us if we ever need to transform the data before doing regression.

* Correlation among various variables: we want to inspect the data for any correlated variables. 
([See Insight](https://data.world/yeeunchoi/f-17-eda-project-3/insights/122dccb6-166f-461c-9f42-ffecf063cfc8))

#### __Scatterplot Matrices__

```{r}
#Scatterplot Matrices
plot(mydata , gap =0)
pairs(~price + room_type+ 
        number_of_reviews+reviews_per_month+
        calculated_host_listings_count+availability_365, data=mydata, 
      main="Simple Scatterplot Matrix")
```
#### __GGPlots of price vs each explanatory variable__
```{r}
ggplot(mydata, aes(minimum_nights, y = price, color = neighbourhood)) + 
  geom_point(size = 2)
ggplot(mydata, aes(minimum_nights, y = price, color = room_type)) + 
  geom_point(size = 2)

ggplot(mydata, aes(number_of_reviews, y = price, color = neighbourhood)) + 
  geom_point(size = 2)
ggplot(mydata, aes(number_of_reviews, y = price, color = room_type)) + 
  geom_point(size = 2)

ggplot(mydata, aes(x=reviews_per_month, y = price, color = neighbourhood)) + 
  geom_point(size = 2)
ggplot(mydata, aes(x=reviews_per_month, y = price, color = room_type)) + 
  geom_point(size = 2)

ggplot(mydata, aes(x=calculated_host_listings_count, y = price, color = neighbourhood)) + 
  geom_point(size = 2)
ggplot(mydata, aes(x=calculated_host_listings_count, y = price, color = room_type)) + 
  geom_point(size = 2)

ggplot(mydata, aes(x=availability_365, y = price, color = neighbourhood)) + 
  geom_point(size = 2)
ggplot(mydata, aes(x=availability_365, y = price, color = room_type)) + 
  geom_point(size = 2)
```

#### __Box Plots__
```{r}
boxplot(price ~ room_type, data = mydata)
boxplot(price ~ neighbourhood, data = mydata)

ggplot(mydata, aes(x = neighbourhood, y = price, fill = room_type)) +
  geom_boxplot(outlier.size = 1.5)
ggplot(data = mydata) +
  geom_point(aes(x = neighbourhood, y = price, color = room_type), size = 2) 
```

#### __Getting Rid of Outliers__
```{r}
ggplot(data = mydata) +
  geom_point(aes(x = latitude, y = longitude, color = neighbourhood), size = 2)
```

From the above latitude and longitude graph, we see an outlier. To correct this, we will take out the zip code 78733. After taking out ZIP 78733, the graph looks like this:

```{r}
mydata1<-mydata[which(mydata$neighbourhood!='78733'),]
ggplot(data = mydata1) +
  geom_point(aes(x = latitude, y = longitude, color = neighbourhood), size = 2)
```

#### __Normality Testing__

Now, we will inspect the normality of the price variable using the Shaprio-Wilk test for normality. Afterwards, we will plot the QQ plot, and inspect the linearity of the plot. Normal QQ plots are relatively linear.

```{r}
# create new dataset without missing data 
df <- na.omit(mydata)
```

```{r,include=FALSE}
detach(mydata)
attach(df)
```
```{r}
ggplot(data = df, aes(sample = price, color = room_type)) +
  stat_qq(cex = 1.2)+labs(title = 'Normality Test for Price',
     x = 'Theoretical Quantiles',y = 'Price')

qqnorm(price)			# Normal probability plot for original variable
shapiro.test(price)
shapiro.test(log(price))

boxcox(price~1,lambda=seq(-5,5,0.01)) # Illustration of Log-Likelihood profile
p<-powerTransform(price)    # Estimaton of Box-Cox lambda [y=(x^lambda-1)/lambda]
p # Write the value of Box-Cox lambda
tprice<-bcPower(price,p$lambda)	# Box-Cox transformation
shapiro.test(tprice)
df$tprice<-tprice
str(df)

```


We ran the Shapiro-Wilk test on the regular dataset and got p<2.2e-16 and a curved QQ plot. This means that the regular dataset is not fit for regression. Then, we transformed the price data into the log(price) and still got p<2.2e-16. Afterwards, we used the Box-Cox transformation to transform price and got a lambda value of -0.15, and p=3.038e-6. This p value is 10^10 times better than the p value for the first two tests. However, this p-value is still pretty low. This is most likely due to the incredibly high degree of freedom ([see insight](https://data.world/yeeunchoi/f-17-eda-project-3/insights/dba5730c-78d7-4517-8d7d-43397db91cb4))Yet, if we plot the transformed price, we get something linear:

```{r}
qqnorm(tprice)			# Normal probability plot for transformed variable
```

Thus, we will run multivariate regression on this transformed price 'tprice' and then select the best model using selection and shrinkage from there.

#__Multiple Linear Regression__
([See Insight #1](https://data.world/yeeunchoi/f-17-eda-project-3/insights/2440a6db-db88-41a4-adf1-7264798404f0),[See Insight #2](https://data.world/yeeunchoi/f-17-eda-project-3/insights/e7b98530-e3b5-446f-9a0b-13cbfa990ea5),[See Insight #3](https://data.world/yeeunchoi/f-17-eda-project-3/insights/3a208e3c-a706-48f5-a0b6-b575a35b1e59))


```{r}
#Fitting a Multiple Regression Model
fit <- lm(tprice ~neighbourhood+latitude*longitude+ room_type+ 
          number_of_reviews+reviews_per_month+minimum_nights+
          calculated_host_listings_count+availability_365, data=df)
summary(fit) # show results
```

From this summary table, we see that minimum nights can be taken out because it is insignificant if we take a 1% significance level. In order to test this, we will test fit1 (full model) vs fit2 (full model without minimum nights). Our hypothesis will be as follows:

H0: Reduced model is a better model (i.e., Model #2)

H1: Full model is needed (i.e., Model #1) 

```{r}
# diagnostic plots 
plot(fit)

# compare models
fit1 <- lm(tprice ~neighbourhood+latitude*longitude+ room_type+minimum_nights+ 
                    number_of_reviews+reviews_per_month+
                    calculated_host_listings_count+availability_365, data=df)
fit2 <- lm(tprice ~neighbourhood+latitude*longitude+ room_type+ 
                    number_of_reviews+reviews_per_month+
                    calculated_host_listings_count+availability_365, data=df)
anova(fit1, fit2)
```

As the P-value is 1.15%, the null hypothesis, H0 is rejected using a significance level of 5% indicating that the full model (Model #1) is more appropriate. Thus, we will keep all 9 variables, and minimum nights will not be taken out. Next, we will try to take out variables using model selection, ridge regression and lasso.

-----------------------------------------------------------------

#__Model Selection__
  Note: Our R code for model selection was working just fine. However, RMarkdown does not like it for whatever reason. Thus, this section will show pictures of the results, with the code and the graphs in the pictures.
```{r}
# classification models [Logistic, LDA,QDA, KNN]
#Define Data Location
srcFile <- 'clean_listings_summary1.csv'
datin <- read.csv(srcFile, header=T)
rdata<-datin[c(6:12,14:16)]
rdata$neighbPIN<-factor(rdata$neighbPIN) #changing to factor
rdata<-rdata[which(rdata$neighbPIN!='78733'),]
rdata=na.omit(rdata)
summary(rdata)
```

```{r,include=FALSE}
detach(df)
attach(rdata)
```

#### __Stepwise Selection__

([See Insight](https://data.world/yeeunchoi/f-17-eda-project-3/insights/4b6e7619-fb7d-44db-9ef7-3f7fddc03475))

> fit <- lm(tprice ~neighbourhood+latitude*longitude+ room_type+minimum_nights+ 
+             number_of_reviews+reviews_per_month+
+             calculated_host_listings_count+availability_365, data=df)
> step <- stepAIC(fit, direction="both")
Start:  AIC=-10469.4
tprice ~ neighbourhood + latitude * longitude + room_type + minimum_nights + 
    number_of_reviews + reviews_per_month + calculated_host_listings_count + 
    availability_365

                                  Df Sum of Sq    RSS      AIC
                                  
<none>                                        241.42 -10469.4

- latitude:longitude              1     0.331 241.75 -10466.2

- minimum_nights                  1     0.409 241.83 -10464.9

- calculated_host_listings_count  1     1.158 242.58 -10453.1

- number_of_reviews               1     2.325 243.74 -10434.7

- availability_365                1     4.929 246.35 -10394.1

- reviews_per_month               1    23.715 265.13 -10112.9

- neighbourhood                  40    35.200 276.62 -10028.7

- room_type                       2   140.802 382.22  -8715.5

> step$anova # display results

* Initial Model:
tprice ~ neighbourhood + latitude * longitude + room_type + minimum_nights + 
    number_of_reviews + reviews_per_month + calculated_host_listings_count + 
    availability_365

* Final Model:
tprice ~ neighbourhood + latitude * longitude + room_type + minimum_nights + 
    number_of_reviews + reviews_per_month + calculated_host_listings_count + 
    availability_365


  Step Df Deviance Resid. Df Resid. Dev      AIC
  
1                       3775   241.4189 -10469.4


* Stepwise selection was not able to change the initial model. Hence, the initial model that contains all of the variables is the best model according to stepwise selection.

#### __Best Subsets Using Leaps__

([See Insight](https://data.world/yeeunchoi/f-17-eda-project-3/insights/64903b9e-7dd4-45f1-990d-2689997782d0))

> regfit.full=regsubsets(price~.,data=rdata, nvmax=9)
> reg.summary=summary(regfit.full)
> names(reg.summary)

[1] "which"  "rsq"    "rss"    "adjr2"  "cp"     "bic"    "outmat" "obj"   

> plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp")

![](cpgraph.png)

plot(regfit.full,scale="Cp")

![](regfitfull.png)


* These are the coefficients of the best model containing 9 variables (5 of 
them belongs to neighborhood and two of them belongs to rmType)  and one constant. It can be noted that out of 40 neighborhood PINs only 5 of them are selected as can also be seen from the following plot. Briefly, this model can be written as:

price~ neighbPIN+rmType+ revPerMonth+ avail365

* If we do Forward Stepwise Selection, we get the same results:

> plot(regfit.fwd,scale="Cp")

![](regfitfull.png)

#### __Validation Error__

([See Insight](https://data.world/yeeunchoi/f-17-eda-project-3/insights/7089de22-69bb-4c4d-a61a-4716d87ae15e))

sqrt(val.errors)

[1] 239.7986 232.5765 229.5339 229.5322 228.0122 228.0008 226.9091 226.1781
[9] 226.4533

> plot(sqrt(val.errors),ylab="Root MSE",ylim=c(200,300),pch=19,type="b")
> points(sqrt(regfit.fwd$rss[-1]/3080),col="blue",pch=19,type="b")
> legend("topright",legend=c("Training","Validation"),col=c("blue","black"),pch=19)

![](validationerror.png)

#### __Cross Validation__

([See Insight](https://data.world/yeeunchoi/f-17-eda-project-3/insights/7089de22-69bb-4c4d-a61a-4716d87ae15e))

> predict.regsubsets=function(object,newdata,id,...){
+   form=as.formula(object$call[[2]])
+   mat=model.matrix(form,newdata)
+   coefi=coef(object,id=id)
+   mat[,names(coefi)]%*%coefi
+ }

> > set.seed(2789)
> folds=sample(rep(1:10,length=nrow(rdata)))
> table(folds)

folds
  1   2   3   4   5   6   7   8   9  10 
383 383 383 383 383 383 382 382 382 382 

> cv.errors=matrix(NA,10,9)
> for(k in 1:10){
+   best.fit=regsubsets(price~.,data=rdata[folds!=k,],nvmax=9,method="forward")
+   for(i in 1:9){
+     pred=predict(best.fit,rdata[folds==k,],id=i)
+     cv.errors[k,i]=mean( (rdata$price[folds==k]-pred)^2)
+   }
+ }

> rmse.cv=sqrt(apply(cv.errors,2,mean))
> plot(rmse.cv,pch=9,type="b")

![](sevenvariable.png)

* This plot is shows that it is possible to obtain a model using 7 variables. It is weird that this shows 7 variables, but we were not able to take away any variables when doing best subset selection.

#### __Ridge Regression__

* ([See Insight](https://data.world/yeeunchoi/f-17-eda-project-3/insights/9002e268-aff5-476d-bff1-25bc0bf38938)) It appears that the dataset doesn't suffers from any collinearity issues. Thus, ridge regression may not be required in this particular case. But we will do ridge regression anyways.

> x=model.matrix(price~.-1,data=rdata) 
> y=rdata$price
> fit.ridge=glmnet(x,y,alpha=0)
> plot(fit.ridge,xvar="lambda",label=TRUE)
> cv.ridge=cv.glmnet(x,y,alpha=0)
> plot(cv.ridge)
> fit.ridge=glmnet(x,y,alpha=0)
> plot(fit.ridge,xvar="lambda",label=TRUE)
> cv.ridge=cv.glmnet(x,y,alpha=0)
> plot(cv.ridge)


![](ridge1.png)


![](ridge2.png)

####__Lasso Method__

* ([See Insight](https://data.world/yeeunchoi/f-17-eda-project-3/insights/7fee63b2-9779-451a-939f-942aeacd9c33))

> fit.lasso=glmnet(x,y)

> plot(fit.lasso,xvar="lambda",label=TRUE)

> cv.lasso=cv.glmnet(x,y)

> plot(cv.lasso)

> coef(cv.lasso)

51 x 1 sparse Matrix of class "dgCMatrix"
                              1
(Intercept)         279.0250281
neighbPIN78701       19.2854990
neighbPIN78702       42.4473047
neighbPIN78703       38.9049960
neighbPIN78704       13.9533243
neighbPIN78705       -5.3721792
neighbPIN78717        .        
neighbPIN78721        .        
neighbPIN78722        .        
neighbPIN78723        .        
neighbPIN78724        .        
neighbPIN78725        .        
neighbPIN78726        .        
neighbPIN78727        .        
neighbPIN78728        .        
neighbPIN78729        .        
neighbPIN78730        7.7861928
neighbPIN78731       12.7856331
neighbPIN78732       39.6132701
neighbPIN78733        .        
neighbPIN78734       55.8608265
neighbPIN78735        .        
neighbPIN78736        .        
neighbPIN78737        .        
neighbPIN78738        .        
neighbPIN78739        .        
neighbPIN78741        .        
neighbPIN78744        .        
neighbPIN78745       -3.1601577
neighbPIN78746        .        
neighbPIN78747        .        
neighbPIN78748        .        
neighbPIN78749        .        
neighbPIN78750        .        
neighbPIN78751      -22.8555069
neighbPIN78752        .        
neighbPIN78753        .        
neighbPIN78754        .        
neighbPIN78756        .        
neighbPIN78757        .        
neighbPIN78758        .        
neighbPIN78759        .        
lat                   .        
long                  .        
rmTypePrivate room -165.7359264
rmTypeShared room  -136.7730788
minNights             .        
numReviews           -0.2461605
revPerMonth         -30.8399847
calHostListCount      .        
avail365              0.1067613

> lasso.tr=glmnet(x[train,],y[train])

> lasso.tr


Call:  glmnet(x = x[train, ], y = y[train]) 

      Df    %Dev   Lambda
 [1,]  0 0.00000 85.66000
 [2,]  1 0.01874 78.05000
 [3,]  1 0.03429 71.12000
 [4,]  2 0.05337 64.80000
 [5,]  2 0.07369 59.05000
 [6,]  2 0.09056 53.80000
 [7,]  2 0.10460 49.02000
 [8,]  2 0.11620 44.67000
 [9,]  2 0.12590 40.70000
[10,]  2 0.13390 37.08000
[11,]  2 0.14050 33.79000
[12,]  2 0.14610 30.79000
[13,]  2 0.15060 28.05000
[14,]  3 0.15580 25.56000
[15,]  4 0.16240 23.29000
[16,]  5 0.16800 21.22000
[17,]  5 0.17350 19.33000
[18,]  7 0.17870 17.62000
[19,]  9 0.18450 16.05000
[20,]  9 0.18990 14.63000
[21,] 13 0.19520 13.33000
[22,] 15 0.20130 12.14000
[23,] 17 0.20750 11.06000
[24,] 17 0.21280 10.08000
[25,] 18 0.21720  9.18600
[26,] 19 0.22100  8.37000
[27,] 21 0.22440  7.62600
[28,] 21 0.22740  6.94900
[29,] 22 0.22990  6.33100
[30,] 23 0.23210  5.76900
[31,] 25 0.23410  5.25600
[32,] 26 0.23590  4.78900
[33,] 26 0.23740  4.36400
[34,] 26 0.23870  3.97600
[35,] 26 0.23970  3.62300
[36,] 26 0.24060  3.30100
[37,] 28 0.24140  3.00800
[38,] 31 0.24200  2.74100
[39,] 32 0.24260  2.49700
[40,] 33 0.24310  2.27500
[41,] 35 0.24360  2.07300
[42,] 36 0.24410  1.88900
[43,] 38 0.24450  1.72100
[44,] 40 0.24490  1.56800
[45,] 41 0.24530  1.42900
[46,] 42 0.24560  1.30200
[47,] 42 0.24590  1.18600
[48,] 43 0.24620  1.08100
[49,] 43 0.24640  0.98490
[50,] 43 0.24660  0.89740
[51,] 44 0.24680  0.81770
[52,] 44 0.24700  0.74510
[53,] 44 0.24710  0.67890
[54,] 46 0.24720  0.61860
[55,] 47 0.24730  0.56360
[56,] 47 0.24750  0.51350
[57,] 47 0.24750  0.46790
[58,] 48 0.24760  0.42640
[59,] 49 0.24770  0.38850
[60,] 49 0.24780  0.35400
[61,] 50 0.24780  0.32250
[62,] 50 0.24790  0.29390
[63,] 50 0.24790  0.26780
[64,] 50 0.24790  0.24400
[65,] 50 0.24790  0.22230
[66,] 50 0.24800  0.20260
[67,] 49 0.24800  0.18460
[68,] 49 0.24800  0.16820
[69,] 49 0.24800  0.15320
[70,] 49 0.24800  0.13960
[71,] 49 0.24800  0.12720
[72,] 49 0.24800  0.11590
[73,] 49 0.24800  0.10560
[74,] 49 0.24810  0.09623
[75,] 50 0.24810  0.08768
[76,] 50 0.24810  0.07989
[77,] 50 0.24810  0.07279
[78,] 49 0.24810  0.06633
[79,] 49 0.24810  0.06043

> pred=predict(lasso.tr,x[-train,])
> rmse= sqrt(apply((y[-train]-pred)^2,2,mean))
> plot(log(lasso.tr$lambda),rmse,type="b",xlab="Log(lambda)")



> lam.best=lasso.tr$lambda[order(rmse)[1]]
> lam.best

[1] 1.889025


> coef(lasso.tr,s=lam.best)

51 x 1 sparse Matrix of class "dgCMatrix"
                               1
(Intercept)        -7789.6851874

neighbPIN78701       102.2931576

neighbPIN78702        98.7847735

neighbPIN78703        89.8817762

neighbPIN78704        55.3953645

neighbPIN78705       -22.4030894

neighbPIN78717         .        

neighbPIN78721       -24.7275688

neighbPIN78722         .        

neighbPIN78723        33.2778159

neighbPIN78724         .        

neighbPIN78725         .        

neighbPIN78726         .        

neighbPIN78727         .        

neighbPIN78728         .        

neighbPIN78729         .        

neighbPIN78730       239.3499163

neighbPIN78731       112.3251769

neighbPIN78732       202.4177402

neighbPIN78733       -61.5877811

neighbPIN78734       231.6376847

neighbPIN78735       -15.7180524

neighbPIN78736        -8.3298800

neighbPIN78737       -28.2790509

neighbPIN78738        -8.8680921

neighbPIN78739         .        

neighbPIN78741        -5.0708995

neighbPIN78744       -26.2402382

neighbPIN78745        -9.9106256

neighbPIN78746        57.5715909

neighbPIN78747         7.5036757

neighbPIN78748         .        

neighbPIN78749         .        

neighbPIN78750         .        

neighbPIN78751       -37.4209792

neighbPIN78752        -4.6343988

neighbPIN78753        24.7771582

neighbPIN78754         .        

neighbPIN78756       -28.0352033

neighbPIN78757       -28.7885433

neighbPIN78758       -23.2282654

neighbPIN78759       -14.6345029

lat                   43.3399357

long                 -68.8331727

rmTypePrivate room  -171.4213512

rmTypeShared room   -181.9969631

minNights              .        

numReviews            -0.5781201

revPerMonth          -33.2594380

calHostListCount      -2.3917369

avail365               0.1609732


![](lasso1.png)


![](lasso2.png)

![](lasso3.png)

# __Logistic Regression__
```{r}
#rdata$rmPrice[rdata$nlprice >0.5] <- "High"
#rdata$rmPrice[rdata$nlprice <= 0.5] <- "Low"
rdata$lprice<-log(rdata$price)
rdata$nlprice=(rdata$lprice-min(rdata$lprice))/(max(rdata$lprice)-min(rdata$lprice))

rmPrice=ifelse(rdata$nlprice>0.5,"High","Low")
#Model Selection
rdata<-cbind(rdata, rmPrice)
rdata$rmPrice<-factor(rdata$rmPrice)
# Logistic regression

glm.fit=glm(rmPrice~rmType+revPerMonth+
            avail365, data=rdata,family=binomial)
summary(glm.fit)
glm.probs=predict(glm.fit,type="response") 
glm.probs[1:5]
summary(glm.probs)
glm.pred=ifelse(glm.probs>0.5,"High","Low")

#attach(rdata)
table(glm.pred,rmPrice)
mean(glm.pred==rmPrice)
# Make training and test set
dim(rdata)
set.seed(1099)#3813,3033
train=sample(seq(3813),3033,replace=FALSE)#about 80% data was selected as training data
length(train)

glm.fit=glm(rmPrice~rmType+revPerMonth+
              avail365, data=rdata,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=rdata[-train,],type="response") 
glm.pred=ifelse(glm.probs >0.5,"High","Low")
test_data=rdata$rmPrice[-train]
table(glm.pred,test_data)
mean(glm.pred==test_data)

```
For logistic regression, we change price to either Low or high. First, we normalized the prices to be in between 0 and 1. Then, we made above a 0.5=High, and less than a 0.5=Low.
The probability of accurately predicting internet usage level was only 23.33%. The probability value are relatively low, as the data is not that strongly related.

#__Linear Discriminant Analysis__

([See Insight](https://data.world/yeeunchoi/f-17-eda-project-3/insights/6e0856a6-a53c-43fb-98ce-9fe7dca2f21e), this insight predicts room type instead of price, but is still a valuable insight)

```{r}
lda.fit=lda(rmPrice~rmType+revPerMonth+
              avail365, data=rdata, subset=train)
lda.fit
plot(lda.fit)
test_data=rdata$rmPrice[-train]
length(test_data)
lda.pred=predict(lda.fit,newdata=rdata[-train,])
data.frame(lda.pred)[1:5,]
table(lda.pred$class,test_data)
mean(lda.pred$class==test_data)
```
To predict the room price as Low or High using three explanatory variables, namely "room type", "reviews per month" and "availability", we produced a linear discriminant analysis model, and then tested its accuracy.

The accuracy of the Linear Discriminant analysis model is 77.18%, which is very high for a bad dataset like this one. This LDA accuracy is due to the transformed price that we did earlier, and also the relatively high prior probability of 62.18% for Low. The confusion matrix shows that we are predicting correctly most of the time.

#__Quadratic Discriminant Analysis__

([See Insight](https://data.world/yeeunchoi/f-17-eda-project-3/insights/452027f0-418f-43e3-b8b2-7dd04da3abe0), this insight attempts to predict room type and not price, but is still a valuable QDA insight)

```{r}
qda.fit=qda(rmPrice~rmType+revPerMonth+
              avail365, data=rdata, subset=train)
qda.fit
test_data=rdata$rmPrice[-train]
length(test_data)
qda.pred=predict(qda.fit,newdata=rdata[-train,])
data.frame(qda.pred)[1:5,]
table(qda.pred$class,test_data)
mean(qda.pred$class==test_data)
```
To predict the room price as Low or High using three explanatory variables, namely "room type", "reviews per month" and "availability", we produced a Quadratic discriminant analysis model, and then tested its accuracy.

The accuracy of the Quadratic Discriminant analysis model is 70.51%. FOr reasons similar to LDA, the confusion matrix shows that we are predicting correctly most of the time.

# __K Nearest Neighbors__

([See Insight #1](https://data.world/yeeunchoi/f-17-eda-project-3/insights/56969cd9-d253-421f-baed-0ba5896b7c42),[See Insight #2](https://data.world/yeeunchoi/f-17-eda-project-3/insights/8e2de7ed-9867-468b-8b70-a6383d66be14),[See Insight #3](https://data.world/yeeunchoi/f-17-eda-project-3/insights/461b1bd4-fd15-4700-b7e7-befe58461d41),[See Insight #4](https://data.world/yeeunchoi/f-17-eda-project-3/insights/94a7553a-99de-42bf-bb91-9bbb6d940a47), these insights attempt to predict room type instead of price, but they are still valuable insights)

```{r}
#attach(rdata)
#Normalize input values as they differ in scale
summary(revPerMonth)
n_revPerMonth=(revPerMonth-min(revPerMonth))/(max(revPerMonth)-min(revPerMonth))
n_avail365=(avail365-min(avail365))/(max(avail365)-min(avail365))
Xval=cbind(n_revPerMonth,n_avail365)
train=c(1:3033)
#Xval[train,]
knn.pred=knn(Xval[train,],Xval[-train,],rmPrice[train],k=1)
table(knn.pred,rmPrice[-train])
mean(knn.pred==rmPrice[-train])
```

The k nearest neighbors model of classification is a method that is based on machine learning that depends on the value of k for accuracy. During the training portion of the algorithm, the alogrithm is trained by looking around using training data that are already classified. The larger the value of k, the larger the search radius that the algorithm will use. In our case, the K-nearest neighbor analysis was able to predict categories by utilizing nearby factors. In our dataset, we want to predict the price of a room by using "reviews per month" and "availability". This K nearest neighbors model was not that accurate, as shown by the confusion matrix, and the 58.33% accuracy.

#__Conclusion__
We used a really bad dataset this project, but managed to get some good results by transforming the price variable so that it is more normally distributed using the Box-Cox transformation. By using this transformed dataset, we got some pretty good results from multiple linear regression with p values less than .05, and t values greater than 2 in magnitude for most of the variables.

  Afterwards, we tried to select which variables to use for the model using best subset selection, stepwise selection, forward selection, ridge regression, and lasso method. From best subset selection, we found that all 9 variables needed to be used in order to create the best model. This was further validated because Bidirectional Stepwise Selection and Forward Stepwise Selection rendered the same results. However, when we run validation errors on the subsets, we see that the Mean Square error for the subset with 7 variables, 8 variables, and 9 variables are all low. This means it should be possible to take off one or two variables from the model. Hence, we tried to shrink the model using ridge regression and lasso. The lasso method was able to shrink the coefficient for the explanatory variable 'numreviews' to -0.57 and 'minimumnights' to 0 and 'availability' to 0.16. Compared to ridge regression, the lasso method creates a better shinked model because it's Mean Square Error Curve is less steep and tends to have a more stable Mean Square Error than that of what ridge regression gives us.

 Then, we transformed the price data from a continuous variable into a categorical variable by normalizing the price data to be in between 0 and 1. Any price above 0.5 was labelled as "High", and rest of the data was labelled "Low". Logistic regression, Linear Discriminant Analysis, and Quadratic Discriminant Analysis were performed using this categorical price data. 
 
 * Logistic Regression had a very poor accuracy of 23.33%, and a poor true detection rate.
 * Linear Discriminant Analysis had a good accuracy of 77.18%, and did good at classifying Low and High correctly.
 * Quadratic Discriminant Analysis had a good accuracy of 70.15%, but a little bit worse than Linear Discriminant Analysis
   
   Afterwards, we conducted k-nearest neighbors analysis, which had an OK 58.33% accuracy.It seems to be good at correctly identifying low prices, but very bad at identiying high prices.
   
  In conclusion, the methodology for analyzing this dataset was very good. However, the dataset itself was not giving any meaningful results. We had to get creative with the dataset, and chose to use a Box-Cox transformation on the price data to normalize the response variable. Moving forward to our next project, we will aim to repeat this methodology we used as a group to dissect the dataset and analyze it, except next time we will use a better dataset and hopefully get better and more clear results.
 
 